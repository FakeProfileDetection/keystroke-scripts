import os
import json
import enum
import pickle
import argparse
import warnings
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np
from tqdm import tqdm

from sklearn.svm import SVC
from extended_minmax import ExtendedMinMaxScalar
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import (
    GridSearchCV,
    StratifiedKFold,
)
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    f1_score,
    precision_score,
    recall_score,
    top_k_accuracy_score,
    confusion_matrix,
)
from xgboost import XGBClassifier, plot_importance
import matplotlib.pyplot as plt
import seaborn as sns
import bob.measure
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.offline as pyo

# Suppress warnings
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

class ScalarType(enum.Enum):
    """Enum class representing the different types of scalers available."""
    STANDARD = 1
    MIN_MAX = 2
    EXTENDED_MIN_MAX = 3
    NONE = 4  # No scaling


# Global configuration and results storage
with open(os.path.join(os.getcwd(), "classifier_config.json"), "r") as f:
    config = json.load(f)

# Add command line argument parsing
parser = argparse.ArgumentParser(description='Run ML experiments with optional early stopping')
parser.add_argument('-e', '--early-stop', action='store_true', 
                   help='Use early stopping for XGBoost and CatBoost models')
args = parser.parse_args()

# Global results storage
EXPERIMENT_RESULTS = []
DETAILED_RESULTS = []  # For comprehensive CSV export
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
EARLY_STOP_SUFFIX = "_early_stop" if args.early_stop else ""
OUTPUT_DIR = Path(f"experiment_results_{TIMESTAMP}{EARLY_STOP_SUFFIX}")
OUTPUT_DIR.mkdir(exist_ok=True)

# Progress tracking
TOTAL_EXPERIMENTS = 0
CURRENT_EXPERIMENT = 0


def validate_data_quality(X_train, X_test, y_train, y_test):
    """Validate data before training"""
    issues = []
    
    # Check for NaN values
    if X_train.isnull().any().any():
        issues.append("X_train contains NaN values")
    if X_test.isnull().any().any():
        issues.append("X_test contains NaN values")
    
    # Check for infinite values
    if not np.isfinite(X_train.select_dtypes(include=[np.number])).all().all():
        issues.append("X_train contains infinite values")
    if not np.isfinite(X_test.select_dtypes(include=[np.number])).all().all():
        issues.append("X_test contains infinite values")
    
    # Check class distribution
    min_samples = y_train.value_counts().min()
    unique_classes = len(y_train.unique())
    
    print(f"üìä Classes: {unique_classes}, Min samples per class: {min_samples}")
    
    if min_samples < 2:
        issues.append("Very low sample count - results may be unreliable")
    
    return len(issues) == 0, issues


def calculate_comprehensive_metrics(y_true, y_pred, y_pred_proba, label_encoder, dataset_type="test"):
    """Calculate comprehensive metrics for user identification task"""
    metrics = {}
    
    # Basic metrics
    metrics[f'{dataset_type}_accuracy'] = accuracy_score(y_true, y_pred)
    metrics[f'{dataset_type}_f1_weighted'] = f1_score(y_true, y_pred, average="weighted", zero_division=0)
    metrics[f'{dataset_type}_f1_macro'] = f1_score(y_true, y_pred, average="macro", zero_division=0)
    metrics[f'{dataset_type}_precision_weighted'] = precision_score(y_true, y_pred, average="weighted", zero_division=0)
    metrics[f'{dataset_type}_precision_macro'] = precision_score(y_true, y_pred, average="macro", zero_division=0)
    metrics[f'{dataset_type}_recall_weighted'] = recall_score(y_true, y_pred, average="weighted", zero_division=0)
    metrics[f'{dataset_type}_recall_macro'] = recall_score(y_true, y_pred, average="macro", zero_division=0)
    
    # Encode labels if needed for top-k calculations
    if isinstance(y_true.iloc[0] if hasattr(y_true, 'iloc') else y_true[0], str):
        y_true_encoded = label_encoder.transform(y_true)
    else:
        y_true_encoded = y_true
    
    # Top-k accuracy (critical for user identification)
    max_k = min(5, len(label_encoder.classes_))
    for k in range(1, max_k + 1):
        try:
            top_k_acc = top_k_accuracy_score(y_true_encoded, y_pred_proba, k=k)
            metrics[f'{dataset_type}_top_{k}_accuracy'] = top_k_acc
        except Exception as e:
            print(f"‚ö†Ô∏è Could not calculate top-{k} accuracy: {e}")
            metrics[f'{dataset_type}_top_{k}_accuracy'] = 0.0
    
    # Recognition rate using bob.measure (handle errors gracefully)
    try:
        rr_scores = []
        for i, true_label in enumerate(y_true_encoded):
            if true_label < y_pred_proba.shape[1]:  # Check bounds
                pos_score = y_pred_proba[i, true_label]
                neg_scores = np.delete(y_pred_proba[i], true_label)
                rr_scores.append((neg_scores, [pos_score]))
        
        if rr_scores:
            recognition_rate = bob.measure.recognition_rate(rr_scores, rank=1)
            metrics[f'{dataset_type}_recognition_rate'] = recognition_rate
        else:
            metrics[f'{dataset_type}_recognition_rate'] = 0.0
    except Exception as e:
        print(f"‚ö†Ô∏è Could not calculate recognition rate: {e}")
        metrics[f'{dataset_type}_recognition_rate'] = 0.0
    
    return metrics


def create_top_k_confusion_matrices(y_true, y_pred_proba, label_encoder, title, filename, k_values=[1, 5]):
    """Create confusion matrices for top-k predictions"""
    
    for k in k_values:
        if k > y_pred_proba.shape[1]:
            continue
            
        # Get top-k predictions
        top_k_indices = np.argsort(y_pred_proba, axis=1)[:, -k:]
        
        # Create binary prediction array (1 if true class in top-k, 0 otherwise)
        y_true_encoded = label_encoder.transform(y_true) if hasattr(y_true, 'iloc') else y_true
        top_k_correct = np.array([true_label in top_k_pred for true_label, top_k_pred in zip(y_true_encoded, top_k_indices)])
        
        # For top-1, use actual predicted class
        if k == 1:
            y_pred_top1 = np.argmax(y_pred_proba, axis=1)
            y_pred_decoded = label_encoder.inverse_transform(y_pred_top1)
            
            # Create confusion matrix with actual user IDs
            cm = confusion_matrix(y_true, y_pred_decoded)
            
            # Get unique labels for proper labeling
            unique_labels = sorted(list(set(y_true) | set(y_pred_decoded)))
            
            plt.figure(figsize=(max(12, len(unique_labels) * 0.5), max(10, len(unique_labels) * 0.4)))
            
            # Only show a subset if too many classes
            if len(unique_labels) > 20:
                # Show top 20 most frequent classes
                top_classes = pd.Series(list(y_true)).value_counts().head(20).index
                mask = np.isin(unique_labels, top_classes)
                cm_subset = cm[np.ix_(mask, mask)]
                labels_subset = [label for i, label in enumerate(unique_labels) if mask[i]]
                
                sns.heatmap(cm_subset, annot=True, fmt='d', cmap='Blues', 
                           xticklabels=labels_subset, yticklabels=labels_subset)
                plt.title(f'Top-{k} Confusion Matrix - {title}\n(Top 20 Most Frequent Users)')
            else:
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                           xticklabels=unique_labels, yticklabels=unique_labels)
                plt.title(f'Top-{k} Confusion Matrix - {title}')
            
            plt.ylabel('True User ID')
            plt.xlabel('Predicted User ID')
            plt.xticks(rotation=45)
            plt.yticks(rotation=0)
            plt.tight_layout()
            plt.savefig(OUTPUT_DIR / f'{filename}_top_{k}_confusion_matrix.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        else:
            # For top-k (k>1), create binary confusion matrix
            # True positive: correct user in top-k, False positive: incorrect prediction
            binary_true = np.ones_like(top_k_correct)  # All should be correctly identified
            binary_pred = top_k_correct.astype(int)
            
            cm_binary = confusion_matrix(binary_true, binary_pred, labels=[0, 1])
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm_binary, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Not in Top-K', 'In Top-K'], 
                       yticklabels=['Should be Found', 'Should be Found'])
            plt.title(f'Top-{k} Identification Success - {title}')
            plt.ylabel('True')
            plt.xlabel('Predicted')
            plt.tight_layout()
            plt.savefig(OUTPUT_DIR / f'{filename}_top_{k}_identification_matrix.png', dpi=300, bbox_inches='tight')
            plt.close()


def create_performance_plots(results_df):
    """Create comprehensive performance plots"""
    if results_df.empty:
        print("‚ö†Ô∏è No results to plot")
        return
        
    # Create subplots
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Top-1 Accuracy Comparison', 'Top-K Accuracy Trends', 
                       'F1 Score vs Top-1 Accuracy', 'Model Performance Overview'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    models = results_df['model'].unique()
    colors = px.colors.qualitative.Set1[:len(models)]
    
    # Top-1 Accuracy comparison
    for i, model in enumerate(models):
        model_data = results_df[results_df['model'] == model]
        if 'test_top_1_accuracy' in model_data.columns:
            fig.add_trace(
                go.Scatter(x=model_data['experiment'], y=model_data['test_top_1_accuracy'],
                          name=f'{model} Top-1', line=dict(color=colors[i]),
                          mode='lines+markers'),
                row=1, col=1
            )
    
    # Top-K accuracy trends
    k_values = [1, 2, 3, 4, 5]
    for i, model in enumerate(models):
        model_data = results_df[results_df['model'] == model].iloc[0:1]  # Take first experiment as example
        k_accuracies = []
        for k in k_values:
            col_name = f'test_top_{k}_accuracy'
            if col_name in model_data.columns:
                k_accuracies.append(model_data[col_name].iloc[0])
            else:
                k_accuracies.append(0)
        
        fig.add_trace(
            go.Scatter(x=k_values, y=k_accuracies,
                      name=f'{model} Top-K', line=dict(color=colors[i]),
                      mode='lines+markers', showlegend=False),
            row=1, col=2
        )
    
    # F1 vs Top-1 Accuracy
    for i, model in enumerate(models):
        model_data = results_df[results_df['model'] == model]
        if 'test_top_1_accuracy' in model_data.columns and 'test_f1_weighted' in model_data.columns:
            fig.add_trace(
                go.Scatter(x=model_data['test_f1_weighted'], y=model_data['test_top_1_accuracy'],
                          name=f'{model}', mode='markers', marker=dict(color=colors[i], size=10),
                          showlegend=False),
                row=2, col=1
            )
    
    # Performance overview (average metrics)
    metrics_summary = []
    for model in models:
        model_data = results_df[results_df['model'] == model]
        avg_top1 = model_data.get('test_top_1_accuracy', pd.Series([0])).mean()
        avg_top5 = model_data.get('test_top_5_accuracy', pd.Series([0])).mean()
        metrics_summary.append({'Model': model, 'Avg_Top1': avg_top1, 'Avg_Top5': avg_top5})
    
    summary_df = pd.DataFrame(metrics_summary)
    fig.add_trace(
        go.Bar(x=summary_df['Model'], y=summary_df['Avg_Top1'], name='Avg Top-1', showlegend=False),
        row=2, col=2
    )
    fig.add_trace(
        go.Bar(x=summary_df['Model'], y=summary_df['Avg_Top5'], name='Avg Top-5', showlegend=False),
        row=2, col=2
    )
    
    fig.update_layout(height=800, title_text="User Identification Performance Overview")
    fig.write_html(OUTPUT_DIR / "performance_plots.html")


def save_model_with_metadata(model, model_name, experiment_name, scaler_type, hyperparams, metrics):
    """Save model with comprehensive metadata"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    early_stop_suffix = "_early_stop" if args.early_stop else ""
    
    # Create filename with all relevant info
    scaler_name = scaler_type.name if hasattr(scaler_type, 'name') else str(scaler_type)
    filename = f"{model_name}_{experiment_name}_{scaler_name}_{timestamp}{early_stop_suffix}.pkl"
    
    # Prepare metadata
    metadata = {
        'model_name': model_name,
        'experiment_name': experiment_name,
        'scaler_type': scaler_name,
        'timestamp': timestamp,
        'early_stopping_used': args.early_stop,
        'hyperparameters': hyperparams,
        'performance_metrics': metrics,
        'config_used': config,
        'task_type': 'user_identification'
    }
    
    # Save model and metadata
    model_data = {
        'model': model,
        'metadata': metadata
    }
    
    filepath = OUTPUT_DIR / filename
    with open(filepath, 'wb') as f:
        pickle.dump(model_data, f)
    
    print(f"üíæ Model saved: {filename}")
    return str(filepath)


def run_xgboost_model(X_train, X_test, y_train, y_test, scalar_obj: ScalarType, 
                     experiment_name="unknown", max_k=5):
    """Enhanced XGBoost model with comprehensive tracking"""
    global CURRENT_EXPERIMENT
    CURRENT_EXPERIMENT += 1
    
    print(f"\nüöÄ Running XGBoost - {experiment_name} - {scalar_obj.name} ({CURRENT_EXPERIMENT}/{TOTAL_EXPERIMENTS})")
    
    # Validate data quality
    is_valid, issues = validate_data_quality(X_train, X_test, y_train, y_test)
    if not is_valid:
        print(f"‚ö†Ô∏è Data quality issues: {issues}")
    
    # Scale the features (commented out as requested - data is pre-normalized)
    # if scalar_obj == ScalarType.MIN_MAX:
    #     scaler = MinMaxScaler()
    #     X_train_scaled = scaler.fit_transform(X_train)
    #     X_test_scaled = scaler.transform(X_test)
    # elif scalar_obj == ScalarType.STANDARD:
    #     scaler = StandardScaler()
    #     X_train_scaled = scaler.fit_transform(X_train)
    #     X_test_scaled = scaler.transform(X_test)
    # elif scalar_obj == ScalarType.EXTENDED_MIN_MAX:
    #     scaler = ExtendedMinMaxScalar()
    #     X_train_scaled = scaler.fit_transform(X_train)
    #     X_test_scaled = scaler.transform(X_test)
    # else:  # NONE
    X_train_scaled = X_train.values
    X_test_scaled = X_test.values

    # Sanity check for NaNs or infinities
    if not np.isfinite(X_train_scaled).all():
        print("‚ö†Ô∏è Warning: X_train_scaled contains NaNs or infinite values, filling with median")
        X_train_scaled = np.nan_to_num(X_train_scaled, nan=np.nanmedian(X_train_scaled))
    if not np.isfinite(X_test_scaled).all():
        print("‚ö†Ô∏è Warning: X_test_scaled contains NaNs or infinite values, filling with median")
        X_test_scaled = np.nan_to_num(X_test_scaled, nan=np.nanmedian(X_test_scaled))

    # Encode the labels
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.transform(y_test)

    min_samples_per_class = y_train.value_counts().min()
    print(f"Minimum samples per class in training set: {min_samples_per_class}")

    # Configure early stopping if requested
    early_stopping_rounds = 50 if args.early_stop else None
    eval_set = [(X_test_scaled, y_test_encoded)] if args.early_stop else None

    # Reduce parallelism to avoid worker timeout issues
    n_jobs = min(4, os.cpu_count() // 2) if os.cpu_count() > 4 else 1

    if min_samples_per_class < 2:
        print("‚ö†Ô∏è Not enough samples for StratifiedKFold ‚Üí fitting XGBoost without CV.")
        best_params = {
            'n_estimators': 1000 if args.early_stop else 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 1.0,
            'colsample_bytree': 1.0,
            'reg_lambda': 1,
            'n_jobs': n_jobs,
        }
        best_xgb = XGBClassifier(**best_params)
        
        with tqdm(desc="Training XGBoost", unit="epoch") as pbar:
            if args.early_stop:
                best_xgb.fit(X_train_scaled, y_train_encoded, 
                             early_stopping_rounds=early_stopping_rounds,
                             eval_set=eval_set, verbose=False)
            else:
                best_xgb.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)
    else:
        stratified_kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)

        param_grid = {
            "n_estimators": [1000] if args.early_stop else [100, 200],
            "max_depth": [4, 6],
            "learning_rate": [0.01, 0.1],
            "subsample": [0.8, 1.0],
            "colsample_bytree": [0.8, 1.0],
            "reg_lambda": [1, 3],
        }

        base_estimator = XGBClassifier(n_jobs=n_jobs)
        
        with tqdm(desc="Grid Search XGBoost") as pbar:
            grid_search_xgb = GridSearchCV(
                base_estimator,
                param_grid,
                scoring="accuracy",
                cv=stratified_kfold,
                n_jobs=n_jobs,
                verbose=0,
            )
            grid_search_xgb.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)
            
        best_xgb = grid_search_xgb.best_estimator_
        best_params = grid_search_xgb.best_params_
        
        # Refit with early stopping if requested
        if args.early_stop:
            best_xgb.set_params(n_estimators=1000)
            with tqdm(desc="Final XGBoost Training") as pbar:
                best_xgb.fit(X_train_scaled, y_train_encoded,
                            early_stopping_rounds=early_stopping_rounds,
                            eval_set=eval_set, verbose=False)
                pbar.update(1)

    # Predictions
    y_pred_train = best_xgb.predict(X_train_scaled)
    y_pred_test = best_xgb.predict(X_test_scaled)
    
    y_pred_train_decoded = label_encoder.inverse_transform(y_pred_train)
    y_pred_test_decoded = label_encoder.inverse_transform(y_pred_test)
    
    y_pred_proba_train = best_xgb.predict_proba(X_train_scaled)
    y_pred_proba_test = best_xgb.predict_proba(X_test_scaled)

    # Calculate comprehensive metrics
    train_metrics = calculate_comprehensive_metrics(
        y_train, y_pred_train_decoded, y_pred_proba_train, label_encoder, "train"
    )
    test_metrics = calculate_comprehensive_metrics(
        y_test, y_pred_test_decoded, y_pred_proba_test, label_encoder, "test"
    )
    
    # Combine metrics
    all_metrics = {**train_metrics, **test_metrics}
    
    # Print key results for user identification
    print(f"XGBoost Train Top-1 Accuracy: {train_metrics.get('train_top_1_accuracy', 0):.4f}")
    print(f"XGBoost Test Top-1 Accuracy: {test_metrics.get('test_top_1_accuracy', 0):.4f}")
    print(f"XGBoost Test Top-5 Accuracy: {test_metrics.get('test_top_5_accuracy', 0):.4f}")
    print(f"Overfitting Check - Top-1 Gap: {train_metrics.get('train_top_1_accuracy', 0) - test_metrics.get('test_top_1_accuracy', 0):.4f}")
    
    # Create confusion matrices for top-1 and top-5
    create_top_k_confusion_matrices(y_test, y_pred_proba_test, label_encoder,
                                   f"XGBoost {experiment_name}", f"xgboost_{experiment_name}")
    
    # Feature importance plot
    if config.get("draw_feature_importance_graph", False):
        plt.figure(figsize=(12, 8))
        plot_importance(best_xgb, importance_type="weight", max_num_features=15)
        plt.title(f"XGBoost Feature Importance - {experiment_name}")
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / f"xgboost_feature_importance_{experiment_name}.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()

    # Save model
    hyperparams = best_params if 'best_params' in locals() else best_xgb.get_params()
    model_path = save_model_with_metadata(
        best_xgb, "XGBoost", experiment_name, scalar_obj, hyperparams, all_metrics
    )
    
    # Store results for reporting
    result_record = {
        'model': 'XGBoost',
        'experiment': experiment_name,
        'scaler': scalar_obj.name,
        'early_stopping': args.early_stop,
        'model_path': model_path,
        'hyperparameters': str(hyperparams),
        **all_metrics
    }
    EXPERIMENT_RESULTS.append(result_record)
    
    # Store detailed results for comprehensive CSV
    for k in range(1, 6):
        detailed_record = {
            'model': 'XGBoost',
            'experiment': experiment_name,
            'scaler': scalar_obj.name,
            'early_stopping': args.early_stop,
            'k_value': k,
            'train_top_k_accuracy': train_metrics.get(f'train_top_{k}_accuracy', 0),
            'test_top_k_accuracy': test_metrics.get(f'test_top_{k}_accuracy', 0),
            'model_path': model_path,
            'hyperparameters': str(hyperparams),
            'timestamp': TIMESTAMP
        }
        DETAILED_RESULTS.append(detailed_record)
    
    return all_metrics


def run_random_forest_model(X_train, X_test, y_train, y_test, scalar_obj: ScalarType, 
                           experiment_name="unknown", max_k=5):
    """Enhanced Random Forest model with comprehensive tracking"""
    global CURRENT_EXPERIMENT
    CURRENT_EXPERIMENT += 1
    
    print(f"\nüå≤ Running Random Forest - {experiment_name} - {scalar_obj.name} ({CURRENT_EXPERIMENT}/{TOTAL_EXPERIMENTS})")
    
    # Validate data quality
    is_valid, issues = validate_data_quality(X_train, X_test, y_train, y_test)
    if not is_valid:
        print(f"‚ö†Ô∏è Data quality issues: {issues}")
    
    # Skip scaling (data is pre-normalized)
    X_train_scaled = X_train.values
    X_test_scaled = X_test.values

    # Encode the labels
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.transform(y_test)
    
    train_min_samples_per_class = y_train.value_counts().min()
    test_min_samples_per_class = y_test.value_counts().min()
    print(f"Minimum samples per class in training set: {train_min_samples_per_class}")
    print(f"Minimum samples per class in test set: {test_min_samples_per_class}")
    
    # Reduce parallelism to avoid worker timeout issues
    n_jobs = min(4, os.cpu_count() // 2) if os.cpu_count() > 4 else 1
    
    if train_min_samples_per_class < 2:
        print("‚ö†Ô∏è Not enough samples for StratifiedKFold ‚Üí fitting without CV.")
        best_rf = RandomForestClassifier(
            n_estimators=500,
            random_state=42,
            n_jobs=n_jobs
        )
        with tqdm(desc="Training Random Forest") as pbar:
            best_rf.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)
        best_params = best_rf.get_params()
    else:
        stratified_kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)

        # Reduced parameter grid to avoid worker timeout
        param_grid = {
            "n_estimators": [100, 300],
            "max_depth": [10, 20, None],
            "min_samples_split": [2, 10],
            "min_samples_leaf": [1, 2],
            "max_features": ["sqrt"],
            "bootstrap": [True],
        }

        with tqdm(desc="Grid Search Random Forest") as pbar:
            grid_search_rf = GridSearchCV(
                RandomForestClassifier(random_state=42, n_jobs=n_jobs),
                param_grid,
                scoring="accuracy",
                cv=stratified_kfold,
                n_jobs=n_jobs,
                verbose=0,
            )
            grid_search_rf.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)

        best_rf = grid_search_rf.best_estimator_
        best_params = grid_search_rf.best_params_

    # Predictions
    y_pred_train = best_rf.predict(X_train_scaled)
    y_pred_test = best_rf.predict(X_test_scaled)
    
    y_pred_train_decoded = label_encoder.inverse_transform(y_pred_train)
    y_pred_test_decoded = label_encoder.inverse_transform(y_pred_test)
    
    y_pred_proba_train = best_rf.predict_proba(X_train_scaled)
    y_pred_proba_test = best_rf.predict_proba(X_test_scaled)

    # Calculate comprehensive metrics
    train_metrics = calculate_comprehensive_metrics(
        y_train, y_pred_train_decoded, y_pred_proba_train, label_encoder, "train"
    )
    test_metrics = calculate_comprehensive_metrics(
        y_test, y_pred_test_decoded, y_pred_proba_test, label_encoder, "test"
    )
    
    # Combine metrics
    all_metrics = {**train_metrics, **test_metrics}
    
    # Print key results for user identification
    print(f"Random Forest Train Top-1 Accuracy: {train_metrics.get('train_top_1_accuracy', 0):.4f}")
    print(f"Random Forest Test Top-1 Accuracy: {test_metrics.get('test_top_1_accuracy', 0):.4f}")
    print(f"Random Forest Test Top-5 Accuracy: {test_metrics.get('test_top_5_accuracy', 0):.4f}")
    print(f"Overfitting Check - Top-1 Gap: {train_metrics.get('train_top_1_accuracy', 0) - test_metrics.get('test_top_1_accuracy', 0):.4f}")
    
    # Create confusion matrices
    create_top_k_confusion_matrices(y_test, y_pred_proba_test, label_encoder,
                                   f"Random Forest {experiment_name}", f"rf_{experiment_name}")

    # Plot feature importance
    if config.get("draw_feature_importance_graph", False):
        plt.figure(figsize=(12, 8))
        feature_importances = best_rf.feature_importances_
        feature_names = X_train.columns
        indices = np.argsort(feature_importances)[-15:]
        
        plt.barh(range(len(indices)), feature_importances[indices], align="center")
        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
        plt.xlabel("Feature Importance")
        plt.title(f"Random Forest Feature Importance - {experiment_name}")
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / f"rf_feature_importance_{experiment_name}.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()

    # Save model
    model_path = save_model_with_metadata(
        best_rf, "RandomForest", experiment_name, scalar_obj, best_params, all_metrics
    )
    
    # Store results for reporting
    result_record = {
        'model': 'RandomForest',
        'experiment': experiment_name,
        'scaler': scalar_obj.name,
        'early_stopping': False,
        'model_path': model_path,
        'hyperparameters': str(best_params),
        **all_metrics
    }
    EXPERIMENT_RESULTS.append(result_record)
    
    # Store detailed results
    for k in range(1, 6):
        detailed_record = {
            'model': 'RandomForest',
            'experiment': experiment_name,
            'scaler': scalar_obj.name,
            'early_stopping': False,
            'k_value': k,
            'train_top_k_accuracy': train_metrics.get(f'train_top_{k}_accuracy', 0),
            'test_top_k_accuracy': test_metrics.get(f'test_top_{k}_accuracy', 0),
            'model_path': model_path,
            'hyperparameters': str(best_params),
            'timestamp': TIMESTAMP
        }
        DETAILED_RESULTS.append(detailed_record)
    
    return all_metrics


def run_catboost_model(X_train, X_test, y_train, y_test, scalar_obj: ScalarType, 
                      experiment_name="unknown", max_k=5):
    """Enhanced CatBoost model with comprehensive tracking"""
    global CURRENT_EXPERIMENT
    CURRENT_EXPERIMENT += 1
    
    print(f"\nüê± Running CatBoost - {experiment_name} - {scalar_obj.name} ({CURRENT_EXPERIMENT}/{TOTAL_EXPERIMENTS})")
    
    # Validate data quality
    is_valid, issues = validate_data_quality(X_train, X_test, y_train, y_test)
    if not is_valid:
        print(f"‚ö†Ô∏è Data quality issues: {issues}")
    
    # Skip scaling (data is pre-normalized)
    X_train_scaled = X_train.values
    X_test_scaled = X_test.values

    # Encode labels
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.transform(y_test)

    min_samples_per_class = y_train.value_counts().min()
    print(f"Minimum samples per class in training set: {min_samples_per_class}")

    # Configure early stopping if requested
    early_stopping_rounds = 50 if args.early_stop else None
    eval_set = Pool(X_test_scaled, y_test_encoded) if args.early_stop else None

    # Reduce thread count to avoid worker issues
    thread_count = min(4, os.cpu_count() // 2) if os.cpu_count() > 4 else 1

    if min_samples_per_class < 2:
        print("‚ö†Ô∏è Not enough samples for StratifiedKFold ‚Üí fitting CatBoost without CV.")
        
        best_params = {
            'iterations': 1000 if args.early_stop else 100,
            'depth': 6,
            'learning_rate': 0.1,
            'l2_leaf_reg': 3,
            'border_count': 64,
            'random_seed': 42,
            'verbose': 0,
            'thread_count': thread_count,
        }
        
        best_catboost_model = CatBoostClassifier(**best_params)
        
        with tqdm(desc="Training CatBoost") as pbar:
            if args.early_stop and eval_set:
                best_catboost_model.fit(
                    X_train_scaled, y_train_encoded,
                    early_stopping_rounds=early_stopping_rounds,
                    eval_set=eval_set,
                    use_best_model=True
                )
            else:
                best_catboost_model.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)
    else:
        stratified_kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)

        # Reduced parameter grid
        param_grid = {
            "iterations": [1000] if args.early_stop else [100, 200],
            "depth": [4, 6],
            "learning_rate": [0.01, 0.1],
            "l2_leaf_reg": [1, 3],
            "border_count": [32, 64],
        }

        with tqdm(desc="Grid Search CatBoost") as pbar:
            grid_search = GridSearchCV(
                estimator=CatBoostClassifier(verbose=0, random_seed=42, thread_count=thread_count),
                param_grid=param_grid,
                scoring="accuracy",
                cv=stratified_kfold,
                verbose=0,
                n_jobs=1,  # CatBoost handles parallelism internally
            )
            grid_search.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)

        best_params = grid_search.best_params_
        best_score = grid_search.best_score_
        print(f"Best Parameters: {best_params}")
        print(f"Best Cross-Validation Score: {best_score}")

        # Train final model with best params
        best_catboost_model = CatBoostClassifier(**best_params, random_seed=42, verbose=0, thread_count=thread_count)
        
        with tqdm(desc="Final CatBoost Training") as pbar:
            if args.early_stop and eval_set:
                best_catboost_model.fit(
                    X_train_scaled, y_train_encoded,
                    early_stopping_rounds=early_stopping_rounds,
                    eval_set=eval_set,
                    use_best_model=True
                )
            else:
                best_catboost_model.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)

    # Predictions
    y_pred_train = best_catboost_model.predict(X_train_scaled)
    y_pred_test = best_catboost_model.predict(X_test_scaled)
    
    y_pred_train_decoded = label_encoder.inverse_transform(y_pred_train)
    y_pred_test_decoded = label_encoder.inverse_transform(y_pred_test)
    
    y_pred_proba_train = best_catboost_model.predict_proba(X_train_scaled)
    y_pred_proba_test = best_catboost_model.predict_proba(X_test_scaled)

    # Calculate comprehensive metrics
    train_metrics = calculate_comprehensive_metrics(
        y_train, y_pred_train_decoded, y_pred_proba_train, label_encoder, "train"
    )
    test_metrics = calculate_comprehensive_metrics(
        y_test, y_pred_test_decoded, y_pred_proba_test, label_encoder, "test"
    )
    
    # Combine metrics
    all_metrics = {**train_metrics, **test_metrics}
    
    # Print key results for user identification
    print(f"CatBoost Train Top-1 Accuracy: {train_metrics.get('train_top_1_accuracy', 0):.4f}")
    print(f"CatBoost Test Top-1 Accuracy: {test_metrics.get('test_top_1_accuracy', 0):.4f}")
    print(f"CatBoost Test Top-5 Accuracy: {test_metrics.get('test_top_5_accuracy', 0):.4f}")
    print(f"Overfitting Check - Top-1 Gap: {train_metrics.get('train_top_1_accuracy', 0) - test_metrics.get('test_top_1_accuracy', 0):.4f}")
    
    # Create confusion matrices
    create_top_k_confusion_matrices(y_test, y_pred_proba_test, label_encoder,
                                   f"CatBoost {experiment_name}", f"catboost_{experiment_name}")

    # Feature importance plot
    if config.get("draw_feature_importance_graph", False):
        try:
            feature_importances = best_catboost_model.get_feature_importance(
                Pool(X_train_scaled, label=y_train_encoded)
            )
            feature_names = X_train.columns
            plt.figure(figsize=(12, 8))
            sorted_indices = feature_importances.argsort()[-15:]
            plt.barh(
                range(len(sorted_indices)),
                feature_importances[sorted_indices],
                align="center",
            )
            plt.yticks(
                range(len(sorted_indices)), [feature_names[i] for i in sorted_indices]
            )
            plt.xlabel("Feature Importance")
            plt.title(f"CatBoost Feature Importance - {experiment_name}")
            plt.tight_layout()
            plt.savefig(OUTPUT_DIR / f"catboost_feature_importance_{experiment_name}.png", 
                       dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"‚ö†Ô∏è Could not generate feature importance plot: {e}")

    # Save model
    hyperparams = best_params if 'best_params' in locals() else best_catboost_model.get_params()
    model_path = save_model_with_metadata(
        best_catboost_model, "CatBoost", experiment_name, scalar_obj, hyperparams, all_metrics
    )
    
    # Store results for reporting
    result_record = {
        'model': 'CatBoost',
        'experiment': experiment_name,
        'scaler': scalar_obj.name,
        'early_stopping': args.early_stop,
        'model_path': model_path,
        'hyperparameters': str(hyperparams),
        **all_metrics
    }
    EXPERIMENT_RESULTS.append(result_record)
    
    # Store detailed results
    for k in range(1, 6):
        detailed_record = {
            'model': 'CatBoost',
            'experiment': experiment_name,
            'scaler': scalar_obj.name,
            'early_stopping': args.early_stop,
            'k_value': k,
            'train_top_k_accuracy': train_metrics.get(f'train_top_{k}_accuracy', 0),
            'test_top_k_accuracy': test_metrics.get(f'test_top_{k}_accuracy', 0),
            'model_path': model_path,
            'hyperparameters': str(hyperparams),
            'timestamp': TIMESTAMP
        }
        DETAILED_RESULTS.append(detailed_record)
    
    return all_metrics


def run_svm_model(X_train, X_test, y_train, y_test, experiment_name="unknown", max_k=5):
    """Enhanced SVM model with comprehensive tracking"""
    global CURRENT_EXPERIMENT
    CURRENT_EXPERIMENT += 1
    
    print(f"\n‚öôÔ∏è Running SVM - {experiment_name} ({CURRENT_EXPERIMENT}/{TOTAL_EXPERIMENTS})")
    
    # Validate data quality
    is_valid, issues = validate_data_quality(X_train, X_test, y_train, y_test)
    if not is_valid:
        print(f"‚ö†Ô∏è Data quality issues: {issues}")
    
    # Skip scaling (data is pre-normalized)
    X_train_scaled = X_train.values
    X_test_scaled = X_test.values

    # Encode labels
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.transform(y_test)

    min_samples_per_class = y_train.value_counts().min()
    print(f"Minimum samples per class in training set: {min_samples_per_class}")

    if min_samples_per_class < 2:
        print("‚ö†Ô∏è Not enough samples for StratifiedKFold ‚Üí fitting SVM without CV.")
        best_svm = SVC(
            C=1,
            kernel="rbf",
            gamma="scale",
            probability=True,
            random_state=42,
        )
        with tqdm(desc="Training SVM") as pbar:
            best_svm.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)
        best_params = best_svm.get_params()
    else:
        stratified_kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)

        # Reduced parameter grid for SVM
        param_grid = {
            "C": [0.1, 1],
            "kernel": ["rbf"],
            "gamma": ["scale"],
        }

        with tqdm(desc="Grid Search SVM") as pbar:
            grid_search_svm = GridSearchCV(
                SVC(probability=True, random_state=42),
                param_grid,
                scoring="accuracy",
                cv=stratified_kfold,
                n_jobs=1,  # SVM doesn't parallelize well
                verbose=0,
            )
            grid_search_svm.fit(X_train_scaled, y_train_encoded)
            pbar.update(1)
            
        best_svm = grid_search_svm.best_estimator_
        best_params = grid_search_svm.best_params_

    # Predictions
    y_pred_train = best_svm.predict(X_train_scaled)
    y_pred_test = best_svm.predict(X_test_scaled)
    
    y_pred_train_decoded = label_encoder.inverse_transform(y_pred_train)
    y_pred_test_decoded = label_encoder.inverse_transform(y_pred_test)
    
    y_pred_proba_train = best_svm.predict_proba(X_train_scaled)
    y_pred_proba_test = best_svm.predict_proba(X_test_scaled)

    # Calculate comprehensive metrics
    train_metrics = calculate_comprehensive_metrics(
        y_train, y_pred_train_decoded, y_pred_proba_train, label_encoder, "train"
    )
    test_metrics = calculate_comprehensive_metrics(
        y_test, y_pred_test_decoded, y_pred_proba_test, label_encoder, "test"
    )
    
    # Combine metrics
    all_metrics = {**train_metrics, **test_metrics}
    
    # Print key results for user identification
    print(f"SVM Train Top-1 Accuracy: {train_metrics.get('train_top_1_accuracy', 0):.4f}")
    print(f"SVM Test Top-1 Accuracy: {test_metrics.get('test_top_1_accuracy', 0):.4f}")
    print(f"SVM Test Top-5 Accuracy: {test_metrics.get('test_top_5_accuracy', 0):.4f}")
    print(f"Overfitting Check - Top-1 Gap: {train_metrics.get('train_top_1_accuracy', 0) - test_metrics.get('test_top_1_accuracy', 0):.4f}")
    
    # Create confusion matrices
    create_top_k_confusion_matrices(y_test, y_pred_proba_test, label_encoder,
                                   f"SVM {experiment_name}", f"svm_{experiment_name}")

    # Feature importance for linear SVM
    if config.get("draw_feature_importance_graph", False) and best_svm.kernel == "linear":
        try:
            coef = best_svm.coef_[0]
            sorted_idx = np.argsort(np.abs(coef))[::-1][:15]
            plt.figure(figsize=(12, 6))
            plt.bar(range(len(sorted_idx)), coef[sorted_idx])
            plt.xticks(range(len(sorted_idx)), X_train.columns[sorted_idx], rotation=45)
            plt.title(f"SVM Linear Coefficients (Top Features) - {experiment_name}")
            plt.tight_layout()
            plt.savefig(OUTPUT_DIR / f"svm_feature_importance_{experiment_name}.png", 
                       dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"‚ö†Ô∏è Could not generate SVM feature importance plot: {e}")
    elif best_svm.kernel != "linear":
        print("‚ö†Ô∏è Note: SVM with non-linear kernel does not provide feature importance.")

    # Save model
    model_path = save_model_with_metadata(
        best_svm, "SVM", experiment_name, ScalarType.NONE, best_params, all_metrics
    )
    
    # Store results for reporting
    result_record = {
        'model': 'SVM',
        'experiment': experiment_name,
        'scaler': 'NONE',
        'early_stopping': False,
        'model_path': model_path,
        'hyperparameters': str(best_params),
        **all_metrics
    }
    EXPERIMENT_RESULTS.append(result_record)
    
    # Store detailed results
    for k in range(1, 6):
        detailed_record = {
            'model': 'SVM',
            'experiment': experiment_name,
            'scaler': 'NONE',
            'early_stopping': False,
            'k_value': k,
            'train_top_k_accuracy': train_metrics.get(f'train_top_{k}_accuracy', 0),
            'test_top_k_accuracy': test_metrics.get(f'test_top_{k}_accuracy', 0),
            'model_path': model_path,
            'hyperparameters': str(best_params),
            'timestamp': TIMESTAMP
        }
        DETAILED_RESULTS.append(detailed_record)
    
    return all_metrics


def generate_html_report():
    """Generate comprehensive HTML report focused on user identification"""
    if not EXPERIMENT_RESULTS:
        print("‚ö†Ô∏è No experiment results to report")
        return
    
    # Convert to DataFrame
    results_df = pd.DataFrame(EXPERIMENT_RESULTS)
    detailed_df = pd.DataFrame(DETAILED_RESULTS)
    
    # Save comprehensive CSV files
    csv_path = OUTPUT_DIR / f"experiment_results_{TIMESTAMP}{EARLY_STOP_SUFFIX}.csv"
    detailed_csv_path = OUTPUT_DIR / f"detailed_topk_results_{TIMESTAMP}{EARLY_STOP_SUFFIX}.csv"
    
    results_df.to_csv(csv_path, index=False)
    detailed_df.to_csv(detailed_csv_path, index=False)
    
    print(f"üìä Results saved to: {csv_path}")
    print(f"üìä Detailed Top-K results saved to: {detailed_csv_path}")
    
    # Create performance plots
    create_performance_plots(results_df)
    
    # Generate HTML report
    best_top1_idx = results_df['test_top_1_accuracy'].fillna(0).idxmax()
    best_top5_idx = results_df['test_top_5_accuracy'].fillna(0).idxmax()
    
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>User Identification Results - {TIMESTAMP}</title>
        <style>
            body {{
                font-family: Arial, sans-serif;
                margin: 40px;
                background-color: #f5f5f5;
            }}
            .header {{
                background-color: #2c3e50;
                color: white;
                padding: 20px;
                border-radius: 10px;
                margin-bottom: 30px;
            }}
            .section {{
                background-color: white;
                padding: 20px;
                margin-bottom: 20px;
                border-radius: 10px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            }}
            table {{
                width: 100%;
                border-collapse: collapse;
                margin-bottom: 20px;
            }}
            th, td {{
                padding: 12px;
                text-align: left;
                border-bottom: 1px solid #ddd;
            }}
            th {{
                background-color: #34495e;
                color: white;
            }}
            .metric-highlight {{
                font-weight: bold;
                color: #27ae60;
            }}
            .warning {{
                color: #e74c3c;
                font-weight: bold;
            }}
            .info-box {{
                background-color: #ecf0f1;
                padding: 15px;
                border-left: 4px solid #3498db;
                margin-bottom: 20px;
            }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>üîê User Identification Results</h1>
            <p><strong>Timestamp:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p><strong>Task:</strong> Cross-Platform User Identification</p>
            <p><strong>Early Stopping:</strong> {'‚úÖ Enabled' if args.early_stop else '‚ùå Disabled'}</p>
            <p><strong>Total Experiments:</strong> {len(results_df)}</p>
            <p><strong>Data Preprocessing:</strong> Pre-normalized (no additional scaling)</p>
        </div>
        
        <div class="section">
            <h2>üéØ Best Performance Summary</h2>
            <div class="info-box">
                <p><strong>Best Top-1 Accuracy:</strong> <span class="metric-highlight">{results_df.loc[best_top1_idx, 'test_top_1_accuracy']:.4f}</span></p>
                <p><strong>Best Top-1 Model:</strong> <span class="metric-highlight">{results_df.loc[best_top1_idx, 'model']} ({results_df.loc[best_top1_idx, 'experiment']})</span></p>
                <p><strong>Best Top-5 Accuracy:</strong> <span class="metric-highlight">{results_df.loc[best_top5_idx, 'test_top_5_accuracy']:.4f}</span></p>
                <p><strong>Best Top-5 Model:</strong> <span class="metric-highlight">{results_df.loc[best_top5_idx, 'model']} ({results_df.loc[best_top5_idx, 'experiment']})</span></p>
            </div>
        </div>
        
        <div class="section">
            <h2>üîç Overfitting Analysis (Top-1 Accuracy)</h2>
            <p>Models with high train-test Top-1 accuracy gap (>0.1) may be overfitting:</p>
    """
    
    # Add overfitting analysis
    results_df['top1_accuracy_gap'] = results_df['train_top_1_accuracy'].fillna(0) - results_df['test_top_1_accuracy'].fillna(0)
    overfitting_models = results_df[results_df['top1_accuracy_gap'] > 0.1]
    
    if len(overfitting_models) > 0:
        html_content += """
            <table>
                <tr>
                    <th>Model</th>
                    <th>Experiment</th>
                    <th>Train Top-1</th>
                    <th>Test Top-1</th>
                    <th>Gap</th>
                </tr>
        """
        for _, row in overfitting_models.iterrows():
            html_content += f"""
                <tr>
                    <td>{row['model']}</td>
                    <td>{row['experiment']}</td>
                    <td>{row['train_top_1_accuracy']:.4f}</td>
                    <td>{row['test_top_1_accuracy']:.4f}</td>
                    <td class="warning">{row['top1_accuracy_gap']:.4f}</td>
                </tr>
            """
        html_content += "</table>"
    else:
        html_content += '<p class="metric-highlight">‚úÖ No significant overfitting detected!</p>'
    
    html_content += """
        </div>
        
        <div class="section">
            <h2>üìä Top-K Performance Summary</h2>
            <table>
                <tr>
                    <th>Model</th>
                    <th>Experiment</th>
                    <th>Top-1</th>
                    <th>Top-2</th>
                    <th>Top-3</th>
                    <th>Top-4</th>
                    <th>Top-5</th>
                </tr>
    """
    
    # Add top-k performance table
    for _, row in results_df.iterrows():
        html_content += f"""
            <tr>
                <td>{row['model']}</td>
                <td>{row['experiment']}</td>
                <td>{row.get('test_top_1_accuracy', 0):.4f}</td>
                <td>{row.get('test_top_2_accuracy', 0):.4f}</td>
                <td>{row.get('test_top_3_accuracy', 0):.4f}</td>
                <td>{row.get('test_top_4_accuracy', 0):.4f}</td>
                <td>{row.get('test_top_5_accuracy', 0):.4f}</td>
            </tr>
        """
    
    html_content += f"""
            </table>
        </div>
        
        <div class="section">
            <h2>üìÅ Generated Files</h2>
            <ul>
                <li>üìä <strong>Summary Results:</strong> experiment_results_{TIMESTAMP}{EARLY_STOP_SUFFIX}.csv</li>
                <li>üìã <strong>Detailed Top-K Results:</strong> detailed_topk_results_{TIMESTAMP}{EARLY_STOP_SUFFIX}.csv</li>
                <li>üìà <strong>Performance Plots:</strong> performance_plots.html</li>
                <li>ü§ñ <strong>Saved Models:</strong> {len([r for r in EXPERIMENT_RESULTS if 'model_path' in r])} models saved with metadata</li>
                <li>üñºÔ∏è <strong>Confusion Matrices:</strong> Top-1 and Top-5 visualization matrices</li>
                <li>üìä <strong>Feature Importance:</strong> Available for tree-based models</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>‚öôÔ∏è Configuration Used</h2>
            <pre>{json.dumps(config, indent=2)}</pre>
        </div>
        
    </body>
    </html>
    """
    
    # Save HTML report
    html_path = OUTPUT_DIR / f"user_identification_report_{TIMESTAMP}{EARLY_STOP_SUFFIX}.html"
    with open(html_path, 'w') as f:
        f.write(html_content)
    
    print(f"üìã HTML report generated: {html_path}")
    print(f"üéØ Open this file in your browser to view the complete report!")
    
    return html_path


def set_total_experiments(total):
    """Set the total number of experiments for progress tracking"""
    global TOTAL_EXPERIMENTS
    TOTAL_EXPERIMENTS = total


# Export the generate_report function so it can be called from the main script
def finalize_experiments():
    """Call this at the end of your experiments to generate reports"""
    if EXPERIMENT_RESULTS:
        generate_html_report()
        print(f"\nüéâ User identification experiments complete!")
        print(f"üìÅ Check the '{OUTPUT_DIR}' folder for all results.")
        print(f"üîç Key files:")
        print(f"   - HTML Report: user_identification_report_{TIMESTAMP}{EARLY_STOP_SUFFIX}.html")
        print(f"   - Detailed CSV: detailed_topk_results_{TIMESTAMP}{EARLY_STOP_SUFFIX}.csv")
        print(f"   - Performance Plots: performance_plots.html")
    else:
        print("‚ö†Ô∏è No experiments were run - no reports generated.")


if __name__ == "__main__":
    print(f"üöÄ User Identification ML Models loaded")
    print(f"‚ö° Early stopping: {args.early_stop}")
    print(f"üìÅ Results will be saved to: {OUTPUT_DIR}")
    print(f"üîß Data preprocessing: Pre-normalized (no additional scaling)")
    